<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Reinforcement Learning Canvas</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #FAF8F5; /* Warm off-white */
            color: #3D4451; /* Soft dark gray for text */
        }
        aside {
            background-color: #F5F2ED; /* Beige sidebar */
        }
        .sidebar-item {
            border-left: 4px solid transparent;
            transition: all 0.2s ease-in-out;
        }
        .sidebar-item.active {
            background-color: #E9E4DB; /* Darker beige */
            color: #000000;
            border-left-color: #4A90E2; /* Accent blue */
            font-weight: 600;
        }
        .sidebar-item:not(.active):hover {
            background-color: #F0ECE5; /* Lighter beige for hover */
        }
        .content-section {
            display: none;
        }
        .content-section.active {
            display: block;
        }
        .card {
            background-color: #ffffff;
            border: 1px solid #EAEAEA;
        }
        canvas {
            background-color: #fcfcfc;
            border-radius: 0.5rem;
            border: 1px solid #e5e7eb;
        }
        .formula {
            background-color: #F5F2ED;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 0.5rem;
            text-align: center;
            font-size: 1.1rem;
            overflow-x: auto;
        }
        .quiz-feedback {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            margin-top: 1rem;
            font-weight: 500;
        }
        .quiz-feedback.correct {
            background-color: #dcfce7; /* green-100 */
            color: #166534; /* green-800 */
        }
        .quiz-feedback.incorrect {
            background-color: #fee2e2; /* red-100 */
            color: #991b1b; /* red-800 */
        }
        .btn-primary {
             background-color: #4A90E2;
             color: white;
        }
        .btn-primary:hover {
            background-color: #357ABD;
        }
        .btn-danger {
            background-color: #D0021B;
            color: white;
        }
        .btn-danger:hover {
            background-color: #B00216;
        }
    </style>
</head>
<body class="flex min-h-screen">

    <!-- Sidebar Navigation -->
    <aside class="w-64 flex-shrink-0 p-6 overflow-y-auto h-screen sticky top-0 border-r border-gray-200">
        <h1 class="text-2xl font-bold text-gray-900 mb-6 px-4">RL Topics</h1>
        <nav id="sidebar-nav" class="space-y-1">
            <!-- Navigation items will be injected by JS -->
        </nav>
    </aside>

    <!-- Main Content -->
    <main id="main-content" class="flex-1 p-8 lg:p-12 overflow-y-auto">
        <!-- Content sections will be injected here -->
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const topics = [
                { id: 'intro-rl', title: 'Introduction to RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Introduction to Reinforcement Learning (RL)</h2>
                    <p class="text-lg text-gray-700 mb-6">Reinforcement Learning is an area of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. Unlike supervised learning, the agent is not told which actions to take, but instead must discover which actions yield the most reward by trying them.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">The Agent-Environment Loop</h3>
                        <p class="text-gray-600 mb-4">The core of RL is the interaction loop between the agent and the environment. At each step, the agent observes a state, takes an action, receives a reward, and observes the next state. This process continues, and the agent's goal is to learn a strategy, or policy, that maximizes its total reward over time.</p>
                        <ul class="list-disc list-inside text-gray-600 space-y-2">
                            <li><strong>Agent:</strong> The learner or decision-maker.</li>
                            <li><strong>Environment:</strong> Everything the agent interacts with.</li>
                            <li><strong>Action (A<sub>t</sub>):</strong> A choice made by the agent.</li>
                            <li><strong>State (S<sub>t</sub>):</strong> The agent's current situation.</li>
                            <li><strong>Reward (R<sub>t</sub>):</strong> Immediate feedback from the environment.</li>
                        </ul>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="intro-rl-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary goal of a Reinforcement Learning agent?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="a" class="mr-2"> Minimize computation time.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="b" class="mr-2"> Maximize cumulative reward.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="c" class="mr-2"> Classify data accurately.</label>
                            <label class="flex items-center"><input type="radio" name="intro-rl-quiz" value="d" class="mr-2"> Follow a predefined path.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'elements-rl', title: 'Elements of RL', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Elements of Reinforcement Learning</h2>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy (π)</h3>
                            <p class="text-gray-600 mb-4">The agent's strategy for selecting actions. It can be deterministic (a specific action for a state) or stochastic (a probability distribution over actions).</p>
                            <div class="formula" data-katex="a = \\pi(s) \\quad \\text{(deterministic)}"></div>
                            <div class="formula mt-2" data-katex="\\pi(a|s) = P[A_t=a | S_t=s] \\quad \\text{(stochastic)}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Reward Signal (R)</h3>
                            <p class="text-gray-600">A scalar feedback from the environment indicating the immediate benefit of an action. The agent's sole objective is to maximize the total reward it receives.</p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Function (V, Q)</h3>
                            <p class="text-gray-600">The expected long-term return from a state or state-action pair, following a particular policy. It quantifies the "goodness" of a state. The return $G_t$ is the sum of discounted future rewards.</p>
                            <div class="formula" data-katex="\\begin{aligned}G_t &= R_{t+1} + \\gamma R_{t+2} + \\dots \\\\ &= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Model of the Environment</h3>
                            <p class="text-gray-600">The agent’s representation of how the environment behaves. It predicts state transitions ($P(s'|s,a)$) and rewards ($R(s,a,s')$). RL methods can be model-based or model-free.</p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="elements-rl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which element of RL defines the agent's strategy or "brain"?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="a" class="mr-2"> Reward Signal</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="b" class="mr-2"> Value Function</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="c" class="mr-2"> Policy</label>
                            <label class="flex items-center"><input type="radio" name="elements-rl-quiz" value="d" class="mr-2"> Model</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mab', title: 'Multi-Armed Bandit', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Multi-Armed Bandit Problem</h2>
                    <p class="text-lg text-gray-700 mb-6">The MAB problem exemplifies the exploration-exploitation tradeoff. The agent must choose between multiple options (arms) to maximize total reward, without knowing which option is best. It must 'exploit' the best-known option but also 'explore' others to find potentially better ones.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Action-Value Estimation</h3>
                        <p class="text-gray-600">We estimate the value of each action $Q(a)$ by averaging the rewards received from that action. The incremental update rule is:</p>
                        <div class="formula" data-katex="Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)"></div>
                        <h3 class="text-xl font-semibold text-gray-900 mt-4 mb-2">ε-Greedy Policy</h3>
                        <p class="text-gray-600">A simple and effective way to balance exploration and exploitation. With probability $1-\epsilon$, we choose the best action (exploit). With probability $\epsilon$, we choose a random action (explore).</p>
                        <div class="formula" data-katex="A_t = \\begin{cases} \\arg\\max_a Q_t(a) & \\text{w.p. } 1 - \\epsilon \\\\ \\text{random action} & \\text{w.p. } \\epsilon \\end{cases}"></div>
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Bandit Simulation</h3>
                        <div class="flex items-center space-x-4 mb-4">
                            <label for="epsilon-slider" class="text-gray-600">Exploration Rate (ε):</label>
                            <input type="range" id="epsilon-slider" min="0" max="1" step="0.01" value="0.1" class="w-48">
                            <span id="epsilon-value" class="font-mono text-blue-600">0.10</span>
                            <button id="reset-bandit-btn" class="btn-danger font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                        <div id="bandit-arms-container" class="flex justify-around items-end h-64 p-4 bg-gray-50 rounded-lg"></div>
                        <div class="mt-4 grid grid-cols-2 gap-4 text-center">
                            <div><p class="text-gray-600">Total Reward</p><p id="bandit-total-reward" class="text-2xl font-bold text-gray-900">0</p></div>
                            <div><p class="text-gray-600">Total Pulls</p><p id="bandit-total-pulls" class="text-2xl font-bold text-gray-900">0</p></div>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mab-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What fundamental tradeoff is the Multi-Armed Bandit problem designed to illustrate?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="a" class="mr-2"> Bias vs. Variance</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="b" class="mr-2"> Exploration vs. Exploitation</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="c" class="mr-2"> Speed vs. Accuracy</label>
                            <label class="flex items-center"><input type="radio" name="mab-quiz" value="d" class="mr-2"> Model-based vs. Model-free</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mdp', title: 'Markov Decision Processes', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">(Finite) Markov Decision Processes (MDP)</h2>
                    <p class="text-lg text-gray-700 mb-6">An MDP is a formal framework for RL problems. A key assumption is the Markov Property: the future is independent of the past given the present. An MDP is defined by a tuple $(S, A, P, R, \gamma)$.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Value Functions & Bellman Equations</h3>
                        <p class="text-gray-600">The state-value function $v_{\pi}(s)$ is the expected return starting from state $s$ and following policy $\pi$.</p>
                        <div class="formula" data-katex="v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]"></div>
                        <p class="text-gray-600 mt-4">The action-value function $q_{\pi}(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$.</p>
                         <div class="formula" data-katex="q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]"></div>
                        <p class="text-gray-600 mt-4">The Bellman expectation equation expresses the value of a state in terms of the values of successor states, creating a recursive relationship:</p>
                        <div class="formula" data-katex="\\begin{aligned}v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} & p(s', r | s, a) \\\\ & \\cdot [r + \\gamma v_\\pi(s')].\\end{aligned}"></div>
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Gridworld</h3>
                        <p class="text-gray-600 mb-4">This Gridworld is an MDP. The agent learns the value of each state by iteratively applying the Bellman equation in an algorithm called Value Iteration.</p>
                        <canvas id="gridworld-canvas" width="400" height="400"></canvas>
                        <div class="mt-4 flex space-x-4">
                            <button id="gridworld-vi-btn" class="btn-primary font-semibold py-2 px-4 rounded">Run Value Iteration</button>
                            <button id="gridworld-reset-btn" class="bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-2 px-4 rounded">Reset</button>
                        </div>
                         <div class="mt-4"><p id="gridworld-iterations" class="text-gray-600">Iterations: 0</p></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mdp-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">The Bellman equation provides a recursive relationship for which of the following?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="a" class="mr-2"> The policy</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="b" class="mr-2"> The reward signal</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="c" class="mr-2"> The value function</label>
                            <label class="flex items-center"><input type="radio" name="mdp-quiz" value="d" class="mr-2"> The state transitions</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'dp', title: 'Dynamic Programming', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Dynamic Programming</h2>
                     <p class="text-lg text-gray-700 mb-6">DP methods solve MDPs given a perfect model of the environment. They use value functions to find optimal policies through bootstrapping.</p>
                    <div class="card p-6 rounded-lg">
                         <h3 class="text-xl font-semibold text-gray-900 mb-2">Policy Iteration</h3>
                         <p class="text-gray-600">This method alternates between two steps:
                            <br>1. <strong>Policy Evaluation:</strong> Iteratively compute the value function for the current policy $\pi$.
                            <br>2. <strong>Policy Improvement:</strong> Act greedily with respect to the new value function to create an improved policy $\pi'$.</p>
                         <h3 class="text-xl font-semibold text-gray-900 mt-4 mb-2">Value Iteration</h3>
                         <p class="text-gray-600">This method directly finds the optimal value function by iteratively applying the Bellman Optimality update. The optimal policy is then extracted from the converged value function.</p>
                        <div class="formula" data-katex="\\begin{aligned}v_{k+1}(s) = \\max_a \\sum_{s', r} & p(s', r | s, a) \\\\ & \\cdot [r + \\gamma v_k(s')].\\end{aligned}"></div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="dp-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is a key requirement for using Dynamic Programming to solve an RL problem?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="a" class="mr-2"> A perfect model of the environment (MDP).</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="b" class="mr-2"> A continuous state space.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="c" class="mr-2"> No knowledge of the environment.</label>
                            <label class="flex items-center"><input type="radio" name="dp-quiz" value="d" class="mr-2"> Experience from complete episodes.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'mc', title: 'Monte Carlo Methods', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Monte Carlo Methods</h2>
                    <p class="text-lg text-gray-700 mb-6">MC methods learn from complete episodes of experience, requiring no model of the environment. The value of a state is estimated by averaging the returns observed after visiting that state.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">First-Visit MC Prediction</h3>
                        <p class="text-gray-600">To evaluate a policy, we run many episodes. For each state $s$ visited in an episode, the return $G_t$ following the <em>first</em> visit to $s$ is used to update $V(s)$.</p>
                        <div class="formula" data-katex="V(s) \\leftarrow \\text{average}(Returns(s))"></div>
                        <h3 class="text-xl font-semibold text-gray-900 mt-6 mb-4">Interactive Return Averaging</h3>
                        <p class="text-gray-600 mb-4">Click the button to simulate an episode. Each episode generates a random return. Observe how the estimated value (the average) converges to the true mean as more episodes are run.</p>
                        <div class="flex items-center space-x-4">
                             <button id="mc-episode-btn" class="btn-primary font-semibold py-2 px-4 rounded">Run Episode</button>
                             <div>
                                <p class="text-gray-600">Estimated Value: <span id="mc-value" class="font-bold text-gray-900">0.00</span></p>
                                <p class="text-gray-600">Episodes: <span id="mc-count" class="font-bold text-gray-900">0</span></p>
                             </div>
                        </div>
                        <div id="mc-returns-log" class="mt-4 p-2 bg-gray-50 h-24 overflow-y-auto rounded border"></div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="mc-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">How do Monte Carlo methods estimate value functions?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="a" class="mr-2"> By bootstrapping from other estimates.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="b" class="mr-2"> By using a model to predict outcomes.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="c" class="mr-2"> By averaging returns from complete episodes.</label>
                            <label class="flex items-center"><input type="radio" name="mc-quiz" value="d" class="mr-2"> By solving the Bellman equation directly.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'td', title: 'Temporal Difference Learning', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Temporal Difference (TD) Learning</h2>
                    <p class="text-lg text-gray-700 mb-6">TD learning is a combination of Monte Carlo and Dynamic Programming. Like MC, it learns from experience without a model. Like DP, it bootstraps, updating its estimates based on other learned estimates without waiting for the end of an episode.</p>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                             <h3 class="text-xl font-semibold text-gray-900 mb-2">SARSA (On-policy)</h3>
                            <p class="text-gray-600">Learns the action-value function based on the policy being followed. The update uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$.</p>
                            <div class="formula" data-katex="\\begin{aligned}Q(S,A) \\leftarrow Q(S,A) + \\alpha \\big[ &R + \\gamma Q(S',A') \\\\ & - Q(S,A) \\big]\\end{aligned}"></div>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Q-Learning (Off-policy)</h3>
                            <p class="text-gray-600">Learns the optimal action-value function, regardless of the agent's actions. It uses the greedy action in the next state for its update.</p>
                             <div class="formula" data-katex="\\begin{aligned}Q(S,A) \\leftarrow Q(S,A) + \\alpha \\big[ &R + \\gamma \\max_{a'} Q(S',a') \\\\ & - Q(S,A) \\big]\\end{aligned}"></div>
                        </div>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="td-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What does it mean for a TD method like Q-Learning to be "off-policy"?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="a" class="mr-2"> It can only learn in environments with no policies.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="b" class="mr-2"> It learns the value of the optimal policy, regardless of the policy it's following.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="c" class="mr-2"> It updates its policy after every single action.</label>
                            <label class="flex items-center"><input type="radio" name="td-quiz" value="d" class="mr-2"> It requires a model of the environment's policy.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'func-approx', title: 'Function Approximation', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Function Approximation</h2>
                     <p class="text-lg text-gray-700 mb-6">For problems with large or continuous state spaces, we approximate the value function with a parameterized function $\\hat{v}(s, \\mathbf{w})$. This allows generalization across states.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Semi-Gradient TD Methods</h3>
                        <p class="text-gray-600">We can use gradient-descent methods to update the function's weight vector $\mathbf{w}$. For TD(0), the update rule is:</p>
                        <div class="formula" data-katex="\\begin{aligned}\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R + &\\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\\\ &\\cdot \\nabla \\hat{v}(S, \\mathbf{w})\\end{aligned}"></div>
                        <p class="text-gray-600 mt-4">This is called "semi-gradient" because it ignores the effect of changing $\mathbf{w}$ on the TD target $R + \\gamma \\hat{v}(S', \\mathbf{w})$.</p>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="fa-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Why is function approximation necessary in some RL problems?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="a" class="mr-2"> To make the rewards higher.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="b" class="mr-2"> To handle very large or continuous state spaces.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="c" class="mr-2"> To guarantee finding the optimal policy.</label>
                            <label class="flex items-center"><input type="radio" name="fa-quiz" value="d" class="mr-2"> To eliminate the need for a discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'deep-rl', title: 'Deep Reinforcement Learning', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Deep Reinforcement Learning</h2>
                     <p class="text-lg text-gray-700 mb-6">Deep RL uses deep neural networks as function approximators, allowing agents to learn from high-dimensional sensory inputs like images.</p>
                     <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">Deep Q-Network (DQN)</h3>
                        <p class="text-gray-600">DQN stabilized learning by using two key techniques: Experience Replay and Fixed Target Networks. The network is trained by minimizing a loss function, typically Mean Squared Error, between the Q-network's prediction and a TD target.</p>
                        <div class="formula" data-katex="\\begin{aligned} L_i(\\theta_i) = \\mathbb{E}&_{(s,a,r,s') \\sim U(D)} \\\\ & \\big[ ( y_i - Q(s,a;\\theta_i) )^2 \\big] \\end{aligned}"></div>
                        <p class="text-gray-600 mt-2">where the target $y_i = r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-)$ is computed with a separate, periodically updated target network with parameters $\\theta_i^-$.</p>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="drl-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What was a key innovation of the Deep Q-Network (DQN) that stabilized training?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="a" class="mr-2"> Using a linear function approximator.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="b" class="mr-2"> Learning the policy directly.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="c" class="mr-2"> Using Experience Replay and a target network.</label>
                            <label class="flex items-center"><input type="radio" name="drl-quiz" value="d" class="mr-2"> Removing the discount factor.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                { id: 'policy-grad', title: 'Policy Gradient Methods', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Policy Gradient Methods</h2>
                    <p class="text-lg text-gray-700 mb-6">These methods directly parameterize and optimize the policy $\\pi(a|s, \\mathbf{\\theta})$. They perform gradient ascent on an objective function $J(\\mathbf{\\theta})$ to find the best policy parameters.</p>
                    <div class="card p-6 rounded-lg">
                        <h3 class="text-xl font-semibold text-gray-900 mb-2">REINFORCE Algorithm</h3>
                        <p class="text-gray-600">A foundational Monte Carlo policy gradient algorithm. The policy parameters are updated in the direction that makes actions leading to high returns more likely.</p>
                        <div class="formula" data-katex="\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha G_t \\nabla \\ln \\pi(A_t|S_t, \\mathbf{\\theta})"></div>
                         <h3 class="text-xl font-semibold text-gray-900 mt-4 mb-2">Actor-Critic Methods</h3>
                        <p class="text-gray-600">Combine policy gradients with value function learning. The "Actor" (policy) learns what to do, while the "Critic" (value function) evaluates how good those actions are. The critic's feedback (e.g., the TD error) is used to train the actor, reducing variance and improving stability.</p>
                    </div>
                     <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="pg-quiz" data-correct-answer="b">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the primary difference between Policy Gradient methods and Value-based methods?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="a" class="mr-2"> Policy Gradient methods only work in deterministic environments.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="b" class="mr-2"> Policy Gradient methods directly optimize a parameterized policy.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="c" class="mr-2"> Value-based methods do not use a discount factor.</label>
                            <label class="flex items-center"><input type="radio" name="pg-quiz" value="d" class="mr-2"> Policy Gradient methods do not use neural networks.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'other', title: 'Other Topics', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Other Topics</h2>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Monte-Carlo Tree Search (MCTS)</h3>
                            <p class="text-gray-600">A powerful planning algorithm that combines the precision of tree search with the generality of random sampling. MCTS doesn't require a model of the environment; instead, it builds a search tree step-by-step based on the outcomes of simulated episodes (rollouts). It's highly effective in games with large branching factors, like Go. The process iterates through four main steps:</p>
                             <ul class="list-decimal list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Selection:</strong> Starting from the root, traverse the tree by selecting the most promising child nodes until a leaf node is reached.</li>
                                <li><strong>Expansion:</strong> Add one or more child nodes to the leaf node, representing new actions to explore.</li>
                                <li><strong>Simulation:</strong> From a new node, run a simulated episode (a "rollout") by choosing random actions until a terminal state is reached. The outcome of this simulation is the return.</li>
                                <li><strong>Backpropagation:</strong> Update the value estimates of all nodes traversed during the selection phase with the return from the simulation.</li>
                            </ul>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">AlphaGo, AlphaGo Zero, & MuZero</h3>
                            <p class="text-gray-600">These landmark systems from DeepMind showcase the power of deep RL and search.
                                <br>• <strong>AlphaGo:</strong> Famously defeated world champion Lee Sedol. It used two neural networks—a "policy network" to select moves and a "value network" to predict the winner—trained on human expert games. These networks guided a powerful MCTS algorithm.
                                <br>• <strong>AlphaGo Zero:</strong> A more powerful version that learned entirely from self-play, without any human data. Starting with random play, it discovered strategies far superior to human play, demonstrating that tabula rasa learning could surpass human knowledge.
                                <br>• <strong>MuZero:</strong> The most general of the three. It masters games like Go, chess, and Atari <span class="italic">without being told the rules</span>. MuZero learns its own model of the environment focused only on the aspects relevant for planning (predicting the next reward, policy, and value), making it a significant step towards more general-purpose algorithms.
                            </p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Imitation Learning</h3>
                            <p class="text-gray-600">A family of methods where an agent learns by observing an expert, rather than through its own trial-and-error. This is useful when exploration is costly, dangerous, or when rewards are hard to define.
                               <br>• <strong>Behavioral Cloning:</strong> The simplest approach. It treats imitation as a supervised learning problem, training a policy network to map states to the actions an expert took in those states. Its main weakness is the "covariate shift" problem: if the agent enters a state the expert never saw, it may not know how to recover.
                               <br>• <strong>Inverse Reinforcement Learning (IRL):</strong> A more advanced approach. Instead of just copying actions, IRL tries to infer the expert's hidden reward function. The assumption is that the expert is acting optimally according to some reward signal. Once this reward function is learned, it can be used with any RL algorithm to train a robust policy.
                            </p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="other-quiz" data-correct-answer="c">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">Which algorithm was famously used by AlphaGo, combining search with learned policies and values?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="other-quiz" value="a" class="mr-2"> Q-Learning</label>
                            <label class="flex items-center"><input type="radio" name="other-quiz" value="b" class="mr-2"> REINFORCE</label>
                            <label class="flex items-center"><input type="radio" name="other-quiz" value="c" class="mr-2"> Monte-Carlo Tree Search (MCTS)</label>
                            <label class="flex items-center"><input type="radio" name="other-quiz" value="d" class="mr-2"> SARSA</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
                 { id: 'special', title: 'Special Sessions', content: `
                    <h2 class="text-3xl font-bold text-gray-900 mb-4 pb-2 border-b border-gray-200">Special Sessions</h2>
                    <div class="space-y-6">
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Multi-Agent RL (MARL)</h3>
                            <p class="text-gray-600">MARL extends RL to systems with multiple agents that learn and act in a shared environment. This introduces significant challenges beyond single-agent RL:
                                <br>• <strong>Non-stationarity:</strong> As other agents learn and change their policies, the environment effectively becomes non-stationary from the perspective of any one agent. What was a good action yesterday might be a bad action today.
                                <br>• <strong>Credit Assignment:</strong> In cooperative settings, it can be difficult to determine which agent's actions were responsible for the team's success or failure.
                                <br>• <strong>Coordination/Competition:</strong> Agents must learn to either coordinate with allies or compete effectively against adversaries.
                                <br>A popular paradigm is "Centralized Training with Decentralized Execution" (CTDE), where agents can share information during training but must act based only on their local observations during execution.
                            </p>
                        </div>
                        <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Human-in-the-Loop (HITL) Learning</h3>
                            <p class="text-gray-600">Also known as Interactive RL, this approach integrates human feedback into the agent's learning loop. This is valuable when environmental rewards are sparse or when we want to align the agent's behavior with complex human preferences. The feedback can take many forms:</p>
                             <ul class="list-disc list-inside text-gray-600 space-y-2 mt-4">
                                <li><strong>Reward Shaping:</strong> A human provides additional reward signals to guide the agent.</li>
                                <li><strong>Policy Shaping:</strong> A human can directly influence the agent's actions or policy.</li>
                                <li><strong>Preference-Based RL:</strong> A human provides feedback by comparing two different behaviors from the agent, and the agent learns a reward function that explains these preferences.</li>
                            </ul>
                        </div>
                         <div class="card p-6 rounded-lg">
                            <h3 class="text-xl font-semibold text-gray-900 mb-2">Safety in RL</h3>
                            <p class="text-gray-600">Ensuring RL agents behave safely and reliably is critical for real-world deployment in areas like robotics, autonomous driving, and healthcare. The goal is to prevent the agent from causing harm or entering catastrophic states, especially during its initial exploration phase. Key approaches include:
                                <br>• <strong>Constrained MDPs:</strong> The problem is formulated to maximize a reward function while satisfying explicit safety constraints (e.g., "keep the robot's temperature below a certain threshold").
                                <br>• <strong>Safe Exploration:</strong> Designing algorithms that explore the environment cautiously, using uncertainty estimates to avoid actions that could lead to dangerous, unknown states.
                                <br>• <strong>Shielding:</strong> Using a predefined set of safety rules or a formal verification module (a "shield") to monitor and override the agent's actions if they are deemed unsafe.
                            </p>
                        </div>
                    </div>
                    <div class="mt-6 card p-6 rounded-lg quiz-container" data-quiz-id="special-quiz" data-correct-answer="a">
                        <h3 class="text-xl font-semibold text-gray-900 mb-3">Quick Quiz</h3>
                        <p class="font-medium text-gray-700 mb-2">What is the main challenge in Multi-Agent RL (MARL) that distinguishes it from single-agent RL?</p>
                        <div class="space-y-2">
                            <label class="flex items-center"><input type="radio" name="special-quiz" value="a" class="mr-2"> The environment becomes non-stationary from a single agent's perspective.</label>
                            <label class="flex items-center"><input type="radio" name="special-quiz" value="b" class="mr-2"> The rewards are always zero.</label>
                            <label class="flex items-center"><input type="radio" name="special-quiz" value="c" class="mr-2"> There are no states, only actions.</label>
                            <label class="flex items-center"><input type="radio" name="special-quiz" value="d" class="mr-2"> It is impossible to use value functions.</label>
                        </div>
                        <button class="quiz-submit-btn mt-4 btn-primary font-semibold py-2 px-4 rounded">Check Answer</button>
                        <div class="quiz-feedback hidden"></div>
                    </div>
                `},
            ];

            function renderKatex() {
                document.querySelectorAll('.formula').forEach(el => {
                    katex.render(el.dataset.katex, el, {
                        throwOnError: false,
                        displayMode: true
                    });
                });
            }

            const mainContent = document.getElementById('main-content');
            const sidebarNav = document.getElementById('sidebar-nav');
            const topicHierarchy = {
                'Introduction': ['intro-rl', 'elements-rl'],
                'Core Problems & Frameworks': ['mab', 'mdp'],
                'Solution Methods': ['dp', 'mc', 'td'],
                'Advanced Topics': ['func-approx', 'deep-rl', 'policy-grad'],
                'Other Topics': ['other'],
                'Special Sessions': ['special']
            };

            // Populate sidebar and content
            Object.entries(topicHierarchy).forEach(([category, topicIds]) => {
                const categoryTitle = document.createElement('h3');
                categoryTitle.className = 'text-xs font-bold text-gray-500 uppercase tracking-wider mt-6 mb-2 px-4';
                categoryTitle.textContent = category;
                sidebarNav.appendChild(categoryTitle);

                topicIds.forEach(topicId => {
                    const topic = topics.find(t => t.id === topicId);
                    if (topic) {
                        const link = document.createElement('a');
                        link.href = '#';
                        link.className = 'block py-2 px-4 rounded-md sidebar-item text-gray-700 text-sm';
                        link.dataset.target = topic.id;
                        link.textContent = topic.title;
                        sidebarNav.appendChild(link);
                        
                        const section = document.createElement('section');
                        section.id = topic.id;
                        section.className = 'content-section';
                        section.innerHTML = topic.content;
                        mainContent.appendChild(section);
                    }
                });
            });

            const navLinks = document.querySelectorAll('.sidebar-item');
            const contentSections = document.querySelectorAll('.content-section');

            function switchTab(targetId) {
                contentSections.forEach(section => {
                    section.classList.toggle('active', section.id === targetId);
                });
                navLinks.forEach(link => {
                    link.classList.toggle('active', link.dataset.target === targetId);
                });
                renderKatex(); // Render formulas on tab switch
                // Initialize canvas if it's visible
                if (targetId === 'mab') initBanditProblem();
                if (targetId === 'mdp') initGridworld();
                if (targetId === 'mc') initMC();
            }

            sidebarNav.addEventListener('click', (e) => {
                if (e.target.closest('.sidebar-item')) {
                    e.preventDefault();
                    const targetId = e.target.closest('.sidebar-item').dataset.target;
                    switchTab(targetId);
                }
            });

            // Quiz Handler using Event Delegation
            mainContent.addEventListener('click', (e) => {
                if (e.target.classList.contains('quiz-submit-btn')) {
                    const quizContainer = e.target.closest('.quiz-container');
                    const quizId = quizContainer.dataset.quizId;
                    const correctAnswer = quizContainer.dataset.correctAnswer;
                    const selectedAnswer = quizContainer.querySelector(`input[name="${quizId}"]:checked`);
                    const feedbackEl = quizContainer.querySelector('.quiz-feedback');
                    
                    if (selectedAnswer) {
                        feedbackEl.classList.remove('hidden', 'correct', 'incorrect');
                        if (selectedAnswer.value === correctAnswer) {
                            feedbackEl.textContent = 'Correct! Well done.';
                            feedbackEl.classList.add('correct');
                        } else {
                            feedbackEl.textContent = 'Not quite. Try again!';
                            feedbackEl.classList.add('incorrect');
                        }
                    } else {
                        feedbackEl.classList.remove('hidden', 'correct');
                        feedbackEl.textContent = 'Please select an answer.';
                        feedbackEl.classList.add('incorrect');
                    }
                }
            });


            // Initial state
            switchTab('intro-rl');

            // --- Multi-Armed Bandit ---
            function initBanditProblem() {
                const armsContainer = document.getElementById('bandit-arms-container');
                if (!armsContainer || armsContainer.dataset.initialized) return;
                armsContainer.dataset.initialized = 'true';
                let intervalId = null;

                const NUM_ARMS = 5;
                let trueMeans = [];
                let estimatedValues = [];
                let pullCounts = [];
                let totalReward = 0;
                let totalPulls = 0;
                let epsilon = 0.1;
                
                const epsilonSlider = document.getElementById('epsilon-slider');
                const epsilonValueSpan = document.getElementById('epsilon-value');
                const resetButton = document.getElementById('reset-bandit-btn');
                const totalRewardEl = document.getElementById('bandit-total-reward');
                const totalPullsEl = document.getElementById('bandit-total-pulls');

                epsilonSlider.addEventListener('input', (e) => {
                    epsilon = parseFloat(e.target.value);
                    epsilonValueSpan.textContent = epsilon.toFixed(2);
                });

                resetButton.addEventListener('click', setup);

                function setup() {
                    if(intervalId) clearInterval(intervalId);
                    armsContainer.innerHTML = '';
                    trueMeans = Array.from({length: NUM_ARMS}, () => Math.random());
                    estimatedValues = Array(NUM_ARMS).fill(0);
                    pullCounts = Array(NUM_ARMS).fill(0);
                    totalReward = 0;
                    totalPulls = 0;

                    for (let i = 0; i < NUM_ARMS; i++) {
                        const armWrapper = document.createElement('div');
                        armWrapper.className = 'flex flex-col items-center h-full justify-end';
                        armWrapper.innerHTML = `
                            <div class="text-sm text-gray-600 mb-1">Pulls: <span id="pulls-${i}">0</span></div>
                            <div class="text-sm font-mono text-blue-600 mb-1">Q(a): <span id="q-val-${i}">0.00</span></div>
                            <div class="w-16 h-4/5 bg-gray-200 rounded-t-lg cursor-pointer hover:bg-gray-300 relative flex items-end justify-center" id="arm-${i}">
                               <div class="bg-blue-500 w-full rounded-t-lg" style="height: 0%;"></div>
                            </div>
                        `;
                        armsContainer.appendChild(armWrapper);
                        document.getElementById(`arm-${i}`).addEventListener('click', () => chooseAndPull(i));
                    }
                    updateDisplay();
                    autoPlay();
                }

                function chooseAndPull(armIndex = -1) {
                    let choice;
                    if (armIndex !== -1) {
                        choice = armIndex;
                    } else {
                        if (Math.random() < epsilon) {
                            choice = Math.floor(Math.random() * NUM_ARMS);
                        } else {
                            const maxQ = Math.max(...estimatedValues);
                            const bestArms = estimatedValues.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                            choice = bestArms[Math.floor(Math.random() * bestArms.length)];
                        }
                    }
                    const reward = Math.random() < trueMeans[choice] ? 1 : 0;
                    totalReward += reward;
                    totalPulls++;
                    pullCounts[choice]++;
                    estimatedValues[choice] += (1 / pullCounts[choice]) * (reward - estimatedValues[choice]);
                    updateDisplay();
                }

                function updateDisplay() {
                    for (let i = 0; i < NUM_ARMS; i++) {
                        document.getElementById(`pulls-${i}`).textContent = pullCounts[i];
                        document.getElementById(`q-val-${i}`).textContent = estimatedValues[i].toFixed(2);
                        const armBar = document.querySelector(`#arm-${i} > div`);
                        armBar.style.height = `${estimatedValues[i] * 100}%`;
                    }
                    totalRewardEl.textContent = totalReward;
                    totalPullsEl.textContent = totalPulls;
                }

                function autoPlay() {
                    intervalId = setInterval(() => {
                        if (document.getElementById('bandit-arms-container')) {
                            chooseAndPull();
                        } else {
                            clearInterval(intervalId);
                        }
                    }, 100);
                }
                setup();
            }

            // --- Monte Carlo ---
            function initMC() {
                 const mcBtn = document.getElementById('mc-episode-btn');
                 if(!mcBtn || mcBtn.dataset.initialized) return;
                 mcBtn.dataset.initialized = 'true';

                 const mcValueEl = document.getElementById('mc-value');
                 const mcCountEl = document.getElementById('mc-count');
                 const mcLogEl = document.getElementById('mc-returns-log');
                 
                 let totalReturn = 0;
                 let count = 0;
                 const TRUE_MEAN = 5;

                 mcBtn.addEventListener('click', () => {
                    count++;
                    const newReturn = TRUE_MEAN + (Math.random() - 0.5) * 4; // Random return around TRUE_MEAN
                    totalReturn += newReturn;
                    const averageReturn = totalReturn / count;
                    
                    mcValueEl.textContent = averageReturn.toFixed(2);
                    mcCountEl.textContent = count;
                    
                    const logEntry = document.createElement('div');
                    logEntry.textContent = `Episode ${count}: Return = ${newReturn.toFixed(2)}`;
                    mcLogEl.appendChild(logEntry);
                    mcLogEl.scrollTop = mcLogEl.scrollHeight;
                 });
            }

            // --- Gridworld ---
            function initGridworld() {
                const canvas = document.getElementById('gridworld-canvas');
                if (!canvas || canvas.dataset.initialized) return;
                canvas.dataset.initialized = 'true';
                let intervalId = null;
                
                const ctx = canvas.getContext('2d');
                const GRID_SIZE = 5;
                const CELL_SIZE = canvas.width / GRID_SIZE;
                
                const grid = [[0, 0, 0, 0, 1], [0, -1, 0, 0, 0], [0, 0, 0, -1, 0], [0, -1, 0, 0, 0], [0, 0, 0, 0, 0]];
                let values = Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0));
                let iterationCount = 0;
                const GAMMA = 0.9;

                const viButton = document.getElementById('gridworld-vi-btn');
                const resetButton = document.getElementById('gridworld-reset-btn');
                const iterationsEl = document.getElementById('gridworld-iterations');

                function drawGrid() {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.strokeStyle = '#e5e7eb';
                            ctx.strokeRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                            if (grid[r][c] === 1) ctx.fillStyle = 'rgba(74, 222, 128, 0.8)';
                            else if (grid[r][c] === -1) ctx.fillStyle = 'rgba(248, 113, 113, 0.8)';
                            else continue;
                            ctx.fillRect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                        }
                    }
                     for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                            ctx.fillStyle = '#1f2937';
                            ctx.font = '16px Inter';
                            ctx.textAlign = 'center';
                            ctx.textBaseline = 'middle';
                            ctx.fillText(values[r][c].toFixed(2), c * CELL_SIZE + CELL_SIZE / 2, r * CELL_SIZE + CELL_SIZE / 2);
                        }
                    }
                }
                
                function valueIterationStep() {
                    let newValues = JSON.parse(JSON.stringify(values));
                    for (let r = 0; r < GRID_SIZE; r++) {
                        for (let c = 0; c < GRID_SIZE; c++) {
                             if (grid[r][c] !== 0) continue;
                            let actionValues = [];
                            const actions = [[-1, 0], [1, 0], [0, -1], [0, 1]];
                            for (const [dr, dc] of actions) {
                                const nr = r + dr, nc = c + dc;
                                let reward = -0.1, nextValue = 0;
                                if (nr >= 0 && nr < GRID_SIZE && nc >= 0 && nc < GRID_SIZE) {
                                     if(grid[nr][nc] === 1) reward = 10;
                                     if(grid[nr][nc] === -1) reward = -10;
                                     nextValue = values[nr][nc];
                                } else {
                                    reward = -1;
                                    nextValue = values[r][c];
                                }
                                actionValues.push(reward + GAMMA * nextValue);
                            }
                            newValues[r][c] = Math.max(...actionValues);
                        }
                    }
                    values = newValues;
                    iterationCount++;
                    iterationsEl.textContent = `Iterations: ${iterationCount}`;
                    drawGrid();
                }

                function runValueIteration() {
                     if(intervalId) clearInterval(intervalId);
                     intervalId = setInterval(() => {
                        if (iterationCount < 50 && document.getElementById('gridworld-canvas')) {
                             valueIterationStep();
                        } else {
                            clearInterval(intervalId);
                        }
                    }, 100);
                }

                function reset() {
                    if(intervalId) clearInterval(intervalId);
                    values = Array(GRID_SIZE).fill(0).map(() => Array(GRID_SIZE).fill(0));
                    iterationCount = 0;
                    iterationsEl.textContent = `Iterations: ${iterationCount}`;
                    drawGrid();
                }

                viButton.addEventListener('click', runValueIteration);
                resetButton.addEventListener('click', reset);

                drawGrid();
            }
        });
    </script>
</body>
</html>



